{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7785e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HDF5_USE_FILE_LOCKING=FALSE\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../models/PFN\")\n",
    "from dataset_pfn import PFNDataset, processed2tau\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from augmented_pfn_model import AugParticleFlowNetwork as Model\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "import os, json\n",
    "from scipy import interpolate\n",
    "%env HDF5_USE_FILE_LOCKING=FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b27d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/avroy/.conda/envs/toptagger_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 30 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "test_path = \"../../datasets/test.h5\"\n",
    "\n",
    "#Loading testing dataset\n",
    "test_set = PFNDataset(test_path, preprocessed=True)\n",
    "testloader = DataLoader(test_set, shuffle=True, batch_size=500, num_workers=30, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbab605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFN_best_v20_augmented\n",
      "PFN_best_baseline_augmented\n",
      "PFN_best_v9_augmented\n",
      "PFN_best_v40_augmented\n",
      "PFN_best_v30_augmented\n"
     ]
    }
   ],
   "source": [
    "all_models = [f for f in os.listdir(\"../../models/PFN/trained_models\") if \"_best\" in f and \"_aug\" in f]\n",
    "print(\"\\n\".join(all_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06588a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval2(model):\n",
    "    labels = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for x,m,y,_ in tqdm(testloader):\n",
    "            x = x.cuda()\n",
    "            m = m.cuda()\n",
    "            pred = model(x, m)\n",
    "            labels.append(y[:,1].cpu().numpy())\n",
    "            preds.append(pred[:,1].cpu().numpy())\n",
    "    labels = np.concatenate(labels, axis=None)\n",
    "    preds = np.concatenate(preds, axis=None)\n",
    "    return labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53572cd5-cb18-4f9e-bee9-112576f4a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalFC(model, data, mask_indices = [-1], mask_values = [0.]):\n",
    "    lfs = data.cuda()\n",
    "    for mask_index, mask_value in zip(mask_indices, mask_values):\n",
    "        if mask_index >= 0:\n",
    "            lfs[:, mask_index] = mask_value*torch.ones(lfs[:, mask_index].shape)  \n",
    "    for ii, layer in enumerate(FcLayers):\n",
    "        if ii == len(FcLayers)-1: \n",
    "            res = layer.forward(lfs).cpu()\n",
    "        elif ii == len(FcLayers)-2:\n",
    "            lfs = layer.forward(lfs)\n",
    "            psmax = lfs.cpu()\n",
    "        else:\n",
    "            lfs = layer.forward(lfs)\n",
    "    return psmax, res\n",
    "\n",
    "def evalPHI(model, test_set, d_mean = [], mask_indices = []):\n",
    "    myDL = DataLoader(test_set, batch_size=1024)\n",
    "    with torch.no_grad():\n",
    "        for idx, (d, m, l, ad) in enumerate(myDL):\n",
    "            # for mask_index in mask_indices:\n",
    "            #     try:\n",
    "            #         d[:, mask_index[0], mask_index[1]] = d_mean[mask_index[0], mask_index[1]]\n",
    "            #     except:\n",
    "            #         d[:, mask_index[0], mask_index[1]] = d_mean[mask_index[1]]\n",
    "            d2 = torch.flatten(d, start_dim=0, end_dim=1)\n",
    "            lfs = model._modules['phi'].forward(d2.cuda())\n",
    "            lfs = torch.stack(torch.split(lfs.permute(1, 0), 200, dim=1), 0)\n",
    "            lfs = lfs * m.cuda().bool().float()\n",
    "            lfs = lfs.sum(-1)\n",
    "            this_lfs = lfs.cpu()\n",
    "            this_taus = processed2tau(d,ad)\n",
    "            psmax, res = evalFC(model, \n",
    "                                torch.cat((lfs, this_taus.cuda()[:,:7]),1).float())\n",
    "            # res = model._modules['fc'].forward(lfs)\n",
    "            if idx == 0:\n",
    "                latents = this_lfs.cpu()\n",
    "                labels = l.cpu()\n",
    "                presoftmax = psmax.cpu()\n",
    "                preds = res.cpu()\n",
    "                aug_data = ad.cpu()\n",
    "                taus = this_taus.cpu()\n",
    "            else:\n",
    "                latents = torch.cat((latents, this_lfs.cpu()), 0)\n",
    "                labels = torch.cat((labels, l.cpu()), 0)\n",
    "                presoftmax  = torch.cat((presoftmax, psmax.cpu()), 0)\n",
    "                preds  = torch.cat((preds, res.cpu()), 0)\n",
    "                aug_data = torch.cat((aug_data, ad.cpu()), 0)\n",
    "                taus = torch.cat((taus, this_taus.cpu()), 0)\n",
    "            if idx == 100:\n",
    "                break\n",
    "    return latents, labels, presoftmax, preds, aug_data, taus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122fadb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 64, 64] [64, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3480776/155672293.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  brr = 1./(1- intrp(0.7))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFN_best_v20_augmented \t\t Params: 17778\t ROC-AUC: 99.99%, Accuracy: 99.93%, BRR: inf\n",
      "[100, 100, 256] [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3480776/155672293.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  brr = 1./(1- intrp(0.7))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFN_best_baseline_augmented \t\t Params: 83158\t ROC-AUC: 99.99%, Accuracy: 99.93%, BRR: inf\n",
      "[100, 100, 64] [64, 64]\n",
      "PFN_best_v9_augmented \t\t Params: 25862\t ROC-AUC: 99.99%, Accuracy: 99.94%, BRR: 26152.00\n",
      "[100, 100, 64] [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3480776/155672293.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  brr = 1./(1- intrp(0.7))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFN_best_v40_augmented \t\t Params: 44566\t ROC-AUC: 100.00%, Accuracy: 99.94%, BRR: inf\n",
      "[100, 100, 128] [100, 100, 100]\n",
      "PFN_best_v30_augmented \t\t Params: 57430\t ROC-AUC: 99.99%, Accuracy: 99.94%, BRR: 52304.00\n"
     ]
    }
   ],
   "source": [
    "#loading model\n",
    "for modelname in all_models:\n",
    "    model_dict = json.load(open(\"../../models/PFN/trained_model_dicts/\" + modelname.replace(\"_best\",\"\") + \".json\"))\n",
    "    #print(modelname, model_dict)\n",
    "    label = model_dict['label']\n",
    "    f_nodes = list(map(int, model_dict['f_nodes'].split(',')))\n",
    "    phi_nodes = list(map(int, model_dict['phi_nodes'].split(',')))\n",
    "\n",
    "    model = Model(input_dims=3, Phi_sizes=phi_nodes, F_sizes=f_nodes).cuda()\n",
    "    model.load_state_dict(torch.load(\"../../models/PFN/trained_models/\" + modelname ))\n",
    "    nparams = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    FcLayers = []\n",
    "    PhiLayers = []\n",
    "    for layer in model._modules['fc']:\n",
    "        #print(layer)\n",
    "        FcLayers.append(layer)\n",
    "    #print('\\n')\n",
    "    for layer in model._modules['phi']:\n",
    "        #print(layer)\n",
    "        PhiLayers.append(layer)\n",
    "    \n",
    "    _, labels, _, preds, _, _ = evalPHI(model, test_set, d_mean = [], mask_indices = [])\n",
    "    labels = labels[:,0]\n",
    "    preds = preds[:,0]\n",
    "    #labels, preds = eval2(model)\n",
    "    accuracy = accuracy_score(labels, preds.round())*100\n",
    "    \n",
    "    auc = roc_auc_score(labels, preds)*100\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(labels, preds, drop_intermediate=False)\n",
    "    intrp = interpolate.interp1d(fpr, tpr)\n",
    "    brr = 1./(1- intrp(0.7))\n",
    "    print(\"{} \\t\\t Params: {}\\t ROC-AUC: {:.2f}%, Accuracy: {:.2f}%, BRR: {:.2f}\".format(modelname,nparams,auc,accuracy,brr))\n",
    "    #print(modelname, \"\\t\", \"ROC-AUC: {:.4f}% Accuracy: {:.4f}%\".format(auc, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-toptagger_env]",
   "language": "python",
   "name": "conda-env-.conda-toptagger_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
